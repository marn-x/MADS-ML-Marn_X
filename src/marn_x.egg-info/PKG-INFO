Metadata-Version: 2.4
Name: marn_x
Version: 0.1.0
Summary: Marnix' version of the MADS_ML module of ADS
License: MIT
Project-URL: Homepage, https://github.com/marn-x/MADS-ML-Marn_X
Project-URL: Documentation, https://github.com/marn-x/MADS-ML-Marn_X/blob/main/README.md
Project-URL: Repository, https://github.com/marn-x/MADS-ML-Marn_X
Project-URL: Issues, https://github.com/marn-x/MADS-ML-Marn_X/issues
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.12.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: hyperopt>=0.2.7
Requires-Dist: jupyter>=1.1.1
Requires-Dist: mads-datasets>=0.3.14
Requires-Dist: mltrainer>=0.2.2
Requires-Dist: pandas>=2.2.3
Requires-Dist: plotly>=6.1.0
Requires-Dist: seaborn>=0.13.2
Requires-Dist: tensorboard>=2.19.0
Requires-Dist: tokenizers>=0.21.1
Requires-Dist: toml>=0.10.2
Requires-Dist: tomlserializer>=0.2.0
Requires-Dist: torch>=2.7.0
Requires-Dist: torch-tb-profiler>=0.4.3
Requires-Dist: torchinfo>=1.8.0
Requires-Dist: torchvision>=0.22.0
Dynamic: license-file

# MADS-ML Marn_X - Marnix Ober 1890946
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A flexible, configuration-driven training framework for PyTorch that minimizes hardcoding and maximizes reproducibility. This framework provides unified experiment tracking across TensorBoard, MLFlow, and Ray Tune, with support for custom datasets, optimizers, and metrics.

## üöÄ Features

- **Configuration-Driven**: All settings managed through TOML/YAML files with environment variable overrides
- **Multiple Backend Support**: Seamlessly switch between TensorBoard, MLFlow, and Ray Tune
- **Customizable**: Component factories for optimizers, schedulers, criterions, and metrics
- **Flexible Data Loading**: Works with both PyTorch DataLoaders and custom data streamers
- **Hyperparameter Tuning**: Integrated Ray Tune support with parameter mapping
- **Automatic Device Management**: Smart detection of CUDA, MPS (Apple Silicon), and CPU
- **Checkpoint Management**: Configurable model saving with best model tracking
- **Early Stopping**: Built-in early stopping with patience and minimum delta
- **MLTrainer Integration**: Optional integration with the mltrainer package
- **Advanced Logging**: Integrated loguru logging with configurable levels and file rotation

## üìã Requirements

- Python 3.12+
- PyTorch 2.0+
- loguru 0.7.0+
- Additional dependencies in `pyproject.toml`

## üîß Installation

### Using UV (Recommended)

UV is a fast Python package manager that provides better dependency resolution and faster installs than pip.

```bash
# Install UV if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone the repository
git clone https://github.com/marn-x/MADS-ML-Marn_X.git
cd MADS-ML-Marn_X

# Create virtual environment and install dependencies
uv sync
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install with optional dependencies
uv sync --dev
```

### Using pip

```bash
# Clone the repository
git clone https://github.com/marn-x/MADS-ML-Marn_X.git
cd MADS-ML-Marn_X

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e .
```

## üéØ Quick Start

### 1. Create a Configuration File

Create a `config.toml` file:

```toml
# Training Configuration
model_name = "resnet18"
epochs = 20
batch_size = 64
learning_rate = 0.001
optimizer_type = "adam"
device = "auto"

# Logging Settings
log_level = "INFO"
log_to_file = true
log_file_path = "training.log"

# Experiment Tracking
experiment_name = "my_first_experiment"
backend = "tensorboard"  # or "mlflow", "ray"
checkpoint_dir = "./checkpoints"

# Early Stopping
early_stopping = true
early_stopping_patience = 5

[optimizer_kwargs]
weight_decay = 1e-5
```

Or generate a sample configuration:

```bash
python -m pytorch_tracker.create_config
```

### 2. Basic Training Script

```python
from pytorch_tracker import (
    TrainingConfig, 
    ConfigLoader,
    track_pytorch_training,
    DataLoaderFactory
)
import torch.nn as nn

# Load configuration
config = ConfigLoader.load_config("config.toml")

# Define your model
model = YourModel()

# Create data loaders
loaders = DataLoaderFactory.create_loaders("eurosat", config)

# Start training with tracking
tracker = track_pytorch_training(
    model=model,
    train_loader=loaders["train"],
    val_loader=loaders["valid"],
    config=config
)
```

### 3. Run Training

```bash
# Using UV
uv run python train.py

# With environment variable overrides
TRAINING_EPOCHS=50 TRAINING_LR=0.0001 uv run python train.py

# With different backend
TRAINING_BACKEND=mlflow uv run python train.py

# With different log level
TRAINING_LOG_LEVEL=DEBUG uv run python train.py
```

## üìä Logging System

### Logging Levels

The framework uses loguru for advanced logging capabilities. Configure the logging level in your `config.toml`:

```toml
log_level = "INFO"  # Default level
```

Available logging levels (from most to least verbose):
- **TRACE**: Extremely detailed information, typically of interest only when diagnosing problems
- **DEBUG**: Detailed information, typically of interest only when diagnosing problems
- **INFO**: Informational messages that confirm things are working as expected
- **SUCCESS**: Successful completion of operations (highlighted in green)
- **WARNING**: Warning messages about potentially harmful situations
- **ERROR**: Error messages about failures that don't stop the program
- **CRITICAL**: Critical errors that might cause the program to abort

### Logging Configuration

```toml
# Logging Settings in config.toml
log_level = "INFO"
log_to_file = true
log_file_path = "training.log"
log_format = "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
log_rotation = "10 MB"  # Rotate when file reaches this size
log_retention = "7 days"  # Keep logs for this duration
log_compression = "zip"  # Compress rotated logs
```

### Example Log Output

```
2024-01-15 14:23:45 | INFO     | __main__:main:42 - Starting training for 20 epochs
2024-01-15 14:23:46 | DEBUG    | pytorch_tracker:get_device:156 - Using CUDA device: NVIDIA GeForce RTX 4060
2024-01-15 14:23:47 | SUCCESS  | pytorch_tracker:train_epoch:234 - Epoch 1/20 completed - Loss: 0.4532, Acc: 82.34%
2024-01-15 14:23:48 | WARNING  | pytorch_tracker:eval_epoch:301 - Early stopping patience: 3/5
```

## üèóÔ∏è Core Classes and Components

### 1. **TrainingConfig**
Central configuration dataclass that holds all training parameters.

```python
from pytorch_tracker import TrainingConfig

config = TrainingConfig(
    epochs=50,
    batch_size=128,
    learning_rate=0.001,
    log_level="DEBUG",  # Set logging level
    backend="mlflow"
)
```

Key attributes:
- Training parameters: `epochs`, `batch_size`, `learning_rate`
- Logging settings: `log_level`, `log_to_file`, `log_format`
- Backend settings: `backend`, `experiment_name`, `tracking_uri`
- Hardware settings: `device`, `num_workers`, `pin_memory`

### 2. **ConfigLoader**
Loads configuration with priority: environment variables > config file > defaults.

```python
from pytorch_tracker import ConfigLoader

# Automatically finds and loads config.toml or config.yaml
config = ConfigLoader.load_config()

# Or specify a path
config = ConfigLoader.load_config("my_config.toml")
```

Environment variable mapping:
- `TRAINING_EPOCHS` ‚Üí `epochs`
- `TRAINING_BATCH_SIZE` ‚Üí `batch_size`
- `TRAINING_LR` ‚Üí `learning_rate`
- `TRAINING_LOG_LEVEL` ‚Üí `log_level`
- `TRAINING_BACKEND` ‚Üí `backend`

### 3. **DeviceManager**
Handles device selection and information.

```python
from pytorch_tracker import DeviceManager

# Automatic device selection
device = DeviceManager.get_device("auto")

# Get device information
info = DeviceManager.get_device_info()
# Returns: {"cuda_available": True, "cuda_device_name": "RTX 4060", ...}
```

### 4. **ComponentFactory**
Creates training components (optimizers, criterions, schedulers) from configuration.

```python
from pytorch_tracker import ComponentFactory

# Create optimizer
optimizer = ComponentFactory.create_optimizer(
    model, 
    "adam", 
    lr=0.001, 
    weight_decay=1e-5
)

# Register custom optimizer
ComponentFactory.register_optimizer("myopt", MyCustomOptimizer)

# Available optimizers
print(ComponentFactory.OPTIMIZERS.keys())
# ['adam', 'sgd', 'adamw', 'rmsprop', 'adagrad', 'adadelta']
```

### 5. **MetricRegistry**
Manages metrics for training evaluation.

```python
from pytorch_tracker import MetricRegistry

# Get a single metric
accuracy = MetricRegistry.get("accuracy")

# Get multiple metrics
metrics = MetricRegistry.get_multiple(["accuracy", "mae"])

# Register custom metric
MetricRegistry.register("custom_metric", MyCustomMetric)
```

### 6. **DataLoaderFactory**
Creates data loaders with a consistent interface.

```python
from pytorch_tracker import DataLoaderFactory

# Create loaders from registered dataset
loaders = DataLoaderFactory.create_loaders("eurosat", config)

# Register custom dataset
def create_custom_loaders(config, preprocessor=None):
    # Your data loading logic
    return {"train": train_loader, "valid": val_loader}

DataLoaderFactory.register_dataset("custom", create_custom_loaders)
```

### 7. **PyTorchTrainingTracker**
Unified tracking interface for different backends.

```python
from pytorch_tracker import PyTorchTrainingTracker

# Initialize tracker
tracker = PyTorchTrainingTracker(config)

# Log hyperparameters
tracker.log_params({"lr": 0.001, "batch_size": 32})

# Log metrics
tracker.log_metrics({"loss": 0.5, "accuracy": 0.95})

# Log model
tracker.log_model(model, optimizer)

# Close when done
tracker.close()
```

### 8. **RayTuneHelper**
Simplifies Ray Tune parameter mapping.

```python
from pytorch_tracker import RayTuneHelper
from ray import tune

# Create search space with short names
search_config = RayTuneHelper.create_search_space(
    lr=tune.loguniform(1e-4, 1e-1),
    bs=tune.choice([32, 64, 128]),
    opt=tune.choice(["adam", "sgd"])
)

# Parameter aliases
print(RayTuneHelper.PARAM_ALIASES)
# {'lr': 'learning_rate', 'bs': 'batch_size', 'opt': 'optimizer_type', ...}
```

## üìö Examples

### Example 1: Custom Dataset Registration

```python
from pytorch_tracker import DataLoaderFactory

def create_custom_loaders(config, preprocessor=None):
    # Your custom data loading logic
    train_loader = ...
    val_loader = ...
    return {"train": train_loader, "valid": val_loader}

# Register your dataset
DataLoaderFactory.register_dataset("my_dataset", create_custom_loaders)

# Use it in config
config.dataset_name = "my_dataset"
```

### Example 2: Hyperparameter Tuning with Ray Tune

```python
from pytorch_tracker import ray_tune_pytorch, RayTuneHelper
from ray import tune

# Define search space
search_config = {
    "learning_rate": tune.loguniform(1e-4, 1e-1),
    "optimizer_type": tune.choice(["adam", "sgd", "adamw"]),
    "batch_size": tune.choice([32, 64, 128]),
}

# Or use short names
search_config = RayTuneHelper.create_search_space(
    lr=tune.loguniform(1e-4, 1e-1),
    bs=tune.choice([32, 64, 128]),
)

# Run tuning
result = ray_tune_pytorch(
    model_fn=create_model,
    data_fn=create_data_loaders,
    config=config,
    search_config=search_config
)
```

### Example 3: MLFlow Tracking

```toml
# config.toml
backend = "mlflow"
tracking_uri = "http://localhost:5000"
log_level = "DEBUG"  # See detailed MLFlow operations

[tags]
project = "image_classification"
team = "research"
```

```bash
# Start MLFlow server
mlflow server --host 0.0.0.0 --port 5000

# Run training with MLFlow tracking
uv run python train.py
```

### Example 4: Custom Optimizer Registration

```python
from pytorch_tracker import ComponentFactory
import torch.optim as optim

# Register custom optimizer
class CustomSGD(optim.SGD):
    def __init__(self, params, lr=0.01, custom_param=0.5, **kwargs):
        super().__init__(params, lr=lr, **kwargs)
        self.custom_param = custom_param

ComponentFactory.register_optimizer("custom_sgd", CustomSGD)

# Use in config
config.optimizer_type = "custom_sgd"
config.optimizer_kwargs = {"custom_param": 0.7, "momentum": 0.9}
```

### Example 5: Advanced Logging Usage

```python
from loguru import logger
from pytorch_tracker import TrainingConfig

# Configure logging through TrainingConfig
config = TrainingConfig(
    log_level="DEBUG",
    log_to_file=True,
    log_file_path="detailed_training.log",
    log_rotation="100 MB",
    log_retention="30 days"
)

# Use logging in your code
logger.debug("Starting data preprocessing")
logger.info("Training epoch {}/{}".format(epoch, total_epochs))
logger.success("Model achieved {:.2f}% accuracy!".format(accuracy))
logger.warning("Learning rate is very low: {:.2e}".format(lr))
logger.error("Failed to save checkpoint: {}".format(error))

# Add custom context
with logger.contextualize(user_id=42, experiment="baseline"):
    logger.info("Running baseline experiment")

# Conditional logging
logger.opt(ansi=True).info("Training <red>failed</red>")
logger.opt(depth=1).info("Called from parent function")
```

## üõ†Ô∏è Advanced Usage

### Configuration Priority

The configuration system follows this priority order:
1. Command-line arguments (if implemented)
2. Environment variables
3. Configuration file (TOML/YAML)
4. Default values

### Environment Variables

```bash
# Training settings
export TRAINING_EPOCHS=100
export TRAINING_BATCH_SIZE=128
export TRAINING_LR=0.0001
export TRAINING_DEVICE=cuda

# Logging settings
export TRAINING_LOG_LEVEL=DEBUG
export TRAINING_LOG_TO_FILE=true

# Backend settings
export TRAINING_BACKEND=mlflow
export TRAINING_EXPERIMENT=my_experiment

# Paths
export TRAINING_CONFIG_PATH=/path/to/config.toml
export TRAINING_CHECKPOINT_DIR=./models
```

### Programmatic Configuration

```python
from pytorch_tracker import TrainingConfig

# Create config programmatically
config = TrainingConfig(
    epochs=50,
    batch_size=256,
    optimizer_type="adamw",
    optimizer_kwargs={
        "lr": 0.001,
        "weight_decay": 1e-4,
        "betas": (0.9, 0.999)
    },
    scheduler_type="cosine",
    scheduler_kwargs={"T_max": 50},
    log_level="INFO",
    log_to_file=True
)

# Save for reproducibility
config.to_toml("experiment_config.toml")
```

### Multi-GPU Training

```python
# Enable DataParallel in config
config.device = "cuda"
config.data_parallel = True

# Or use DistributedDataParallel
# (requires additional setup)
```

### Custom Training Loop with Logging

```python
from pytorch_tracker import track_pytorch_training
from loguru import logger

# Define custom callbacks
def on_epoch_end(epoch, metrics):
    logger.info(f"Epoch {epoch} metrics: {metrics}")
    
    if metrics["val_accuracy"] > 0.95:
        logger.success("Reached target accuracy!")

callbacks = {
    "on_epoch_end": on_epoch_end
}

# Train with callbacks
tracker = track_pytorch_training(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    config=config,
    callbacks=callbacks
)
```

## üìä Viewing Results

### TensorBoard
```bash
tensorboard --logdir runs/
```

### MLFlow
```bash
mlflow ui
```

### Ray Tune Dashboard
```bash
# Ray dashboard is automatically available during tuning
# Default URL: http://localhost:8265
```

### Log Files
```bash
# View logs in real-time
tail -f training.log

# View colored logs
cat training.log | less -R

# Search logs
grep "ERROR" training.log
grep "Epoch.*completed" training.log
```

## üß™ Testing

```bash
# Run tests with UV
uv run pytest tests/

# With coverage
uv run pytest --cov=pytorch_tracker tests/

# Run with specific log level
TRAINING_LOG_LEVEL=DEBUG uv run pytest tests/ -v
```

## üîç Debugging Tips

1. **Enable DEBUG logging** to see detailed information:
   ```toml
   log_level = "DEBUG"
   ```

2. **Use TRACE level** for extremely detailed debugging:
   ```toml
   log_level = "TRACE"
   ```

3. **Check log files** for errors that might not appear in console:
   ```bash
   grep -i error training.log
   ```

4. **Use logger context** for tracking specific operations:
   ```python
   with logger.contextualize(task="data_loading"):
       logger.info("Loading dataset")
   ```

5. **Enable exception catching** in loguru:
   ```python
   logger.opt(exception=True).error("An error occurred")
   ```

## ü§ù Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Install with dev dependencies
uv pip install -e ".[dev]"

# Run formatter
black src/

# Run linter
ruff src/

# Run type checker
mypy src/
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Built on top of PyTorch's training best practices
- Inspired by PyTorch Lightning's configuration system
- Uses MLFlow for experiment tracking
- Leverages Ray Tune for hyperparameter optimization
- Powered by loguru for advanced logging capabilities

## üìû Support

- Create an issue for bug reports or feature requests
- Check the [examples](examples/) directory for more use cases
- See [docs](docs/) for detailed API documentation

## üó∫Ô∏è Roadmap

- [ ] Distributed training support (DDP)
- [ ] Automatic mixed precision (AMP) training
- [ ] Integration with Weights & Biases
- [ ] Support for more dataset formats
- [ ] CLI tool for common operations
- [ ] Model zoo with pretrained weights
- [ ] Advanced visualization tools
- [ ] Automated hyperparameter optimization
- [ ] Model pruning and quantization support
- [ ] Experiment comparison tools

---

Made by Marnix
